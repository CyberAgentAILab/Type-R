From fe17c17fee6faf6a970f2fc636419afbc6e97094 Mon Sep 17 00:00:00 2001
From: shimoda-uec <shimoda-k@mm.inf.uec.ac.jp>
Date: Wed, 28 May 2025 00:17:53 +0900
Subject: [PATCH] patch

---
 vlmeval/dataset/mmalignbench.py               | 172 +++++++++---------
 .../utils/megabench/aggregation/__init__.py   |   1 +
 .../utils/megabench/parsing/__init__.py       |   1 +
 .../megabench/parsing/common/__init__.py      |   1 +
 .../utils/megabench/scoring/__init__.py       |   1 +
 .../megabench/scoring/common/__init__.py      |   1 +
 vlmeval/dataset/wildvision.py                 | 115 ++++++------
 7 files changed, 146 insertions(+), 146 deletions(-)
 create mode 100644 vlmeval/dataset/utils/megabench/aggregation/__init__.py
 create mode 100644 vlmeval/dataset/utils/megabench/parsing/__init__.py
 create mode 100644 vlmeval/dataset/utils/megabench/parsing/common/__init__.py
 create mode 100644 vlmeval/dataset/utils/megabench/scoring/__init__.py
 create mode 100644 vlmeval/dataset/utils/megabench/scoring/common/__init__.py

diff --git a/vlmeval/dataset/mmalignbench.py b/vlmeval/dataset/mmalignbench.py
index 6d8c6bb..09a4b44 100644
--- a/vlmeval/dataset/mmalignbench.py
+++ b/vlmeval/dataset/mmalignbench.py
@@ -2,11 +2,10 @@
 import re
 from functools import partial
 
-from .image_base import ImageBaseDataset
-from .utils import build_judge, DEBUG_MESSAGE
 from ..smp import *
 from ..utils import track_progress_rich
-
+from .image_base import ImageBaseDataset
+from .utils import DEBUG_MESSAGE, build_judge
 
 SYSTEM_PROMPT = """\
 Please act as an impartial evaluator and assess the quality of the responses provided by two AI assistants to a given user prompt and accompanying image. You will be provided with Assistant A's and Assistant B's answers. Your task is to determine which assistant's response is superior.
@@ -78,7 +77,7 @@ PROMPT_TEMPLATE_GT = """**INPUT**:
 """
 
 
-REGEX_PATTERN = re.compile("\[\[([AB<>=]+)\]\]")  # noqa: W605
+REGEX_PATTERN = re.compile(r"\[\[([AB<>=]+)\]\]")  # noqa: W605
 
 
 def get_score(judgement, pattern=REGEX_PATTERN):
@@ -93,24 +92,26 @@ def get_score(judgement, pattern=REGEX_PATTERN):
 
 
 def MMAlignBench_auxeval(model, line):
-    if 'gt' in line and str(line['gt']) != 'nan':
-        config = dict(question=line['question'], gt=line['gt'], answer_1=line['A'], answer_2=line['B'])
-        prompt = SYSTEM_PROMPT_GT + '\n' + PROMPT_TEMPLATE_GT.format(**config)
+    if "gt" in line and str(line["gt"]) != "nan":
+        config = dict(
+            question=line["question"],
+            gt=line["gt"],
+            answer_1=line["A"],
+            answer_2=line["B"],
+        )
+        prompt = SYSTEM_PROMPT_GT + "\n" + PROMPT_TEMPLATE_GT.format(**config)
         # prompt = PROMPT_TEMPLATE.format(**config)
-        print('gt_prompt'+prompt)
+        print("gt_prompt" + prompt)
     else:
-        config = dict(question=line['question'], answer_1=line['A'], answer_2=line['B'])
-        prompt = SYSTEM_PROMPT + '\n' + PROMPT_TEMPLATE.format(**config)
+        config = dict(question=line["question"], answer_1=line["A"], answer_2=line["B"])
+        prompt = SYSTEM_PROMPT + "\n" + PROMPT_TEMPLATE.format(**config)
         # prompt = PROMPT_TEMPLATE.format(**config)
-        print('prompt'+prompt)
+        print("prompt" + prompt)
 
-    prefix = 'data:image/jpeg;base64,'
-    img = prefix + line['image']
+    prefix = "data:image/jpeg;base64,"
+    img = prefix + line["image"]
 
-    messages = [
-        dict(type='text', value=prompt),
-        dict(type='image', value=img)
-    ]
+    messages = [dict(type="text", value=prompt), dict(type="image", value=img)]
 
     retry = 2
     while retry:
@@ -121,22 +122,18 @@ def MMAlignBench_auxeval(model, line):
         retry -= 1
 
     if score is None:
-        return 'Unknown'
+        return "Unknown"
     return [score, resp]
 
 
 class MMAlignBench(ImageBaseDataset):
-    TYPE = 'VQA'
-    DATASET_URL = {'MMAlignBench': 'https://opencompass.openxlab.space/utils/VLMEval/MMAlignBench.tsv'}
-    DATASET_MD5 = {'MMAlignBench': 'd00d8e61c99257cbaf76d8d5e926f01e'}
-
-    score_map = {
-        'A>>B': -2,
-        'A>B': -1,
-        'A=B': 0,
-        'B>A': 1,
-        'B>>A': 2
+    TYPE = "VQA"
+    DATASET_URL = {
+        "MMAlignBench": "https://opencompass.openxlab.space/utils/VLMEval/MMAlignBench.tsv"
     }
+    DATASET_MD5 = {"MMAlignBench": "d00d8e61c99257cbaf76d8d5e926f01e"}
+
+    score_map = {"A>>B": -2, "A>B": -1, "A=B": 0, "B>A": 1, "B>>A": 2}
 
     # Given one data record, return the built prompt (a multi-modal message), can override
     def build_prompt(self, line):
@@ -144,59 +141,60 @@ class MMAlignBench(ImageBaseDataset):
             line = self.data.iloc[line]
 
         if self.meta_only:
-            tgt_path = toliststr(line['image_path'])
+            tgt_path = toliststr(line["image_path"])
         else:
             tgt_path = self.dump_image(line)
 
-        question = line['question']
+        question = line["question"]
 
         msgs = []
         if isinstance(tgt_path, list):
-            msgs.extend([dict(type='image', value=p) for p in tgt_path])
+            msgs.extend([dict(type="image", value=p) for p in tgt_path])
         else:
-            msgs = [dict(type='image', value=tgt_path)]
+            msgs = [dict(type="image", value=tgt_path)]
         # WildVision adopts text first
-        msgs = [dict(type='text', value=question)] + msgs
+        msgs = [dict(type="text", value=question)] + msgs
         return msgs
 
     @classmethod
     def gen_eval_base(self, eval_file, b64_map):
         data = load(eval_file)
-        data['B'] = data.pop('prediction')
-        data['A'] = data.pop('claude3_sonnet')
-        data['image'] = [b64_map[x] for x in data['index']]
+        data["B"] = data.pop("prediction")
+        data["A"] = data.pop("claude3_sonnet")
+        data["image"] = [b64_map[x] for x in data["index"]]
         return data
 
     # It returns a DataFrame
     @classmethod
     def evaluate(self, eval_file, **judge_kwargs):
         # We adopt pairwise evaluation (twice for a pair) for this dataset
-        suffix = eval_file.split('.')[-1]
-        model = judge_kwargs['model']
-        storage = eval_file.replace(f'.{suffix}', f'_{model}.xlsx')
-        score_file = eval_file.replace(f'.{suffix}', f'_{model}_score.csv')
-        tmp_file = eval_file.replace(f'.{suffix}', f'_{model}.pkl')
-        nproc = judge_kwargs.pop('nproc', 4)
+        suffix = eval_file.split(".")[-1]
+        model = judge_kwargs["model"]
+        storage = eval_file.replace(f".{suffix}", f"_{model}.xlsx")
+        score_file = eval_file.replace(f".{suffix}", f"_{model}_score.csv")
+        tmp_file = eval_file.replace(f".{suffix}", f"_{model}.pkl")
+        nproc = judge_kwargs.pop("nproc", 4)
 
         if not osp.exists(storage):
-            raw_data = MMAlignBench('MMAlignBench').data
-            b64_map = {x: y for x, y in zip(raw_data['index'], raw_data['image'])}
+            raw_data = MMAlignBench("MMAlignBench").data
+            b64_map = {x: y for x, y in zip(raw_data["index"], raw_data["image"])}
             data = self.gen_eval_base(eval_file, b64_map)
 
             # judge_kwargs['system_prompt'] = SYSTEM_PROMPT
-            judge_kwargs['temperature'] = 0
-            judge_kwargs['img_detail'] = 'high'
-            judge_kwargs['timeout'] = 300
+            judge_kwargs["temperature"] = 0
+            judge_kwargs["img_detail"] = "high"
+            judge_kwargs["timeout"] = 300
             model = build_judge(max_tokens=4096, **judge_kwargs)
 
             assert model.working(), (
-                'MMAlignBench evaluation requires a working OPENAI API\n' + DEBUG_MESSAGE
+                "MMAlignBench evaluation requires a working OPENAI API\n"
+                + DEBUG_MESSAGE
             )
 
             lt = len(data)
             lines = [data.iloc[i] for i in range(lt)]
             tups = [(model, line) for line in lines]
-            indices = [line['index'] for line in lines]
+            indices = [line["index"] for line in lines]
 
             ans = load(tmp_file) if osp.exists(tmp_file) else {}
             tups = [x for x, i in zip(tups, indices) if i not in ans]
@@ -213,14 +211,14 @@ class MMAlignBench(ImageBaseDataset):
                 )
                 ans = load(tmp_file)
                 for k, v in zip(indices, new_results):
-                    ans[k] = {'score': v[0], 'resp': v[1]}
+                    ans[k] = {"score": v[0], "resp": v[1]}
             else:
-                for k,v in ans.items():
-                    ans[k] = {'score': v[0], 'resp': v[1]}
+                for k, v in ans.items():
+                    ans[k] = {"score": v[0], "resp": v[1]}
             # breakpoint()
-            data['score'] = [ans[x]['score'] for x in data['index']]
-            data['judge'] = [ans[x]['resp'] for x in data['index']]
-            data.pop('image')
+            data["score"] = [ans[x]["score"] for x in data["index"]]
+            data["judge"] = [ans[x]["resp"] for x in data["index"]]
+            data.pop("image")
             dump(data, storage)
 
         data = load(storage)
@@ -231,31 +229,33 @@ class MMAlignBench(ImageBaseDataset):
 
         for i in range(lt):
             item = data.iloc[i]
-            if item['score'] not in self.score_map:
+            if item["score"] not in self.score_map:
                 score = 0
             else:
-                score = self.score_map[item['score']]
-                if '_rev' in item['index']:
+                score = self.score_map[item["score"]]
+                if "_rev" in item["index"]:
                     score = -score
             scores[score] += 1
-            type = item['type']
+            type = item["type"]
             type_scores[type][score] += 1
 
         name_map = {
-            2: 'Much Better',
-            1: 'Better',
-            0: 'Tie',
-            -1: 'Worse',
-            -2: 'Much Worse'
+            2: "Much Better",
+            1: "Better",
+            0: "Tie",
+            -1: "Worse",
+            -2: "Much Worse",
         }
         scores = {name_map[k]: v for k, v in scores.items()}
-        scores['Reward'] = (
-            100 * scores.get('Much Better', 0)
-            + 50 * scores.get('Better', 0)
-            - 50 * scores.get('Worse', 0)
-            - 100 * scores.get('Much Worse', 0)
+        scores["Reward"] = (
+            100 * scores.get("Much Better", 0)
+            + 50 * scores.get("Better", 0)
+            - 50 * scores.get("Worse", 0)
+            - 100 * scores.get("Much Worse", 0)
+        ) / lt
+        scores["Win Rate"] = (
+            scores.get("Better", 0) + scores.get("Much Better", 0)
         ) / lt
-        scores['Win Rate'] = (scores.get('Better', 0) + scores.get('Much Better', 0)) / lt
         scores = {k: [v] for k, v in scores.items()}
         scores = pd.DataFrame(scores)
 
@@ -263,20 +263,24 @@ class MMAlignBench(ImageBaseDataset):
             type_score_dict = {name_map[k]: v for k, v in type_score_dict.items()}
             type_lt = sum(type_score_dict.values())
 
-            type_score_dict['Reward'] = (
+            type_score_dict["Reward"] = (
                 (
-                    100 * type_score_dict.get('Much Better', 0)
-                    + 50 * type_score_dict.get('Better', 0)
-                    - 50 * type_score_dict.get('Worse', 0)
-                    - 100 * type_score_dict.get('Much Worse', 0)
+                    100 * type_score_dict.get("Much Better", 0)
+                    + 50 * type_score_dict.get("Better", 0)
+                    - 50 * type_score_dict.get("Worse", 0)
+                    - 100 * type_score_dict.get("Much Worse", 0)
                 )
                 / type_lt
                 if type_lt > 0
                 else 0
             )
 
-            type_score_dict['Win Rate'] = (
-                (type_score_dict.get('Better', 0) + type_score_dict.get('Much Better', 0)) / type_lt
+            type_score_dict["Win Rate"] = (
+                (
+                    type_score_dict.get("Better", 0)
+                    + type_score_dict.get("Much Better", 0)
+                )
+                / type_lt
                 if type_lt > 0
                 else 0
             )
@@ -284,13 +288,13 @@ class MMAlignBench(ImageBaseDataset):
             # 将该类型的得分添加到结果中
             type_score_df = pd.DataFrame(
                 {
-                    f"{type_name}_Much Better": [type_score_dict.get('Much Better', 0)],
-                    f"{type_name}_Better": [type_score_dict.get('Better', 0)],
-                    f"{type_name}_Tie": [type_score_dict.get('Tie', 0)],
-                    f"{type_name}_Worse": [type_score_dict.get('Worse', 0)],
-                    f"{type_name}_Much Worse": [type_score_dict.get('Much Worse', 0)],
-                    f"{type_name}_Reward": [type_score_dict['Reward']],
-                    f"{type_name}_Win Rate": [type_score_dict['Win Rate']],
+                    f"{type_name}_Much Better": [type_score_dict.get("Much Better", 0)],
+                    f"{type_name}_Better": [type_score_dict.get("Better", 0)],
+                    f"{type_name}_Tie": [type_score_dict.get("Tie", 0)],
+                    f"{type_name}_Worse": [type_score_dict.get("Worse", 0)],
+                    f"{type_name}_Much Worse": [type_score_dict.get("Much Worse", 0)],
+                    f"{type_name}_Reward": [type_score_dict["Reward"]],
+                    f"{type_name}_Win Rate": [type_score_dict["Win Rate"]],
                 }
             )
             scores = pd.concat([scores, type_score_df], axis=1)
diff --git a/vlmeval/dataset/utils/megabench/aggregation/__init__.py b/vlmeval/dataset/utils/megabench/aggregation/__init__.py
new file mode 100644
index 0000000..8b13789
--- /dev/null
+++ b/vlmeval/dataset/utils/megabench/aggregation/__init__.py
@@ -0,0 +1 @@
+
diff --git a/vlmeval/dataset/utils/megabench/parsing/__init__.py b/vlmeval/dataset/utils/megabench/parsing/__init__.py
new file mode 100644
index 0000000..8b13789
--- /dev/null
+++ b/vlmeval/dataset/utils/megabench/parsing/__init__.py
@@ -0,0 +1 @@
+
diff --git a/vlmeval/dataset/utils/megabench/parsing/common/__init__.py b/vlmeval/dataset/utils/megabench/parsing/common/__init__.py
new file mode 100644
index 0000000..8b13789
--- /dev/null
+++ b/vlmeval/dataset/utils/megabench/parsing/common/__init__.py
@@ -0,0 +1 @@
+
diff --git a/vlmeval/dataset/utils/megabench/scoring/__init__.py b/vlmeval/dataset/utils/megabench/scoring/__init__.py
new file mode 100644
index 0000000..8b13789
--- /dev/null
+++ b/vlmeval/dataset/utils/megabench/scoring/__init__.py
@@ -0,0 +1 @@
+
diff --git a/vlmeval/dataset/utils/megabench/scoring/common/__init__.py b/vlmeval/dataset/utils/megabench/scoring/common/__init__.py
new file mode 100644
index 0000000..8b13789
--- /dev/null
+++ b/vlmeval/dataset/utils/megabench/scoring/common/__init__.py
@@ -0,0 +1 @@
+
diff --git a/vlmeval/dataset/wildvision.py b/vlmeval/dataset/wildvision.py
index b1ad1fd..22f062d 100644
--- a/vlmeval/dataset/wildvision.py
+++ b/vlmeval/dataset/wildvision.py
@@ -1,11 +1,9 @@
 import re
-from functools import partial
 
-from .image_base import ImageBaseDataset
-from .utils import build_judge, DEBUG_MESSAGE
 from ..smp import *
 from ..utils import track_progress_rich
-
+from .image_base import ImageBaseDataset
+from .utils import DEBUG_MESSAGE, build_judge
 
 SYSTEM_PROMPT = """\
 Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user \
@@ -49,7 +47,7 @@ PROMPT_TEMPLATE = """\
 """
 
 
-REGEX_PATTERN = re.compile("\[\[([AB<>=]+)\]\]")  # noqa: W605
+REGEX_PATTERN = re.compile(r"\[\[([AB<>=]+)\]\]")  # noqa: W605
 
 
 def get_score(judgement, pattern=REGEX_PATTERN):
@@ -64,16 +62,13 @@ def get_score(judgement, pattern=REGEX_PATTERN):
 
 
 def WildVision_auxeval(model, line):
-    config = dict(question=line['question'], answer_1=line['A'], answer_2=line['B'])
+    config = dict(question=line["question"], answer_1=line["A"], answer_2=line["B"])
     prompt = PROMPT_TEMPLATE.format(**config)
 
-    prefix = 'data:image/jpeg;base64,'
-    img = prefix + line['image']
+    prefix = "data:image/jpeg;base64,"
+    img = prefix + line["image"]
 
-    messages = [
-        dict(type='text', value=prompt),
-        dict(type='image', value=img)
-    ]
+    messages = [dict(type="text", value=prompt), dict(type="image", value=img)]
 
     retry = 2
     while retry:
@@ -84,24 +79,18 @@ def WildVision_auxeval(model, line):
         retry -= 1
 
     if score is None:
-        return 'Unknown'
+        return "Unknown"
     return score
 
 
 class WildVision(ImageBaseDataset):
-    TYPE = 'VQA'
+    TYPE = "VQA"
     DATASET_URL = {
-        'WildVision': 'https://opencompass.openxlab.space/utils/VLMEval/WildVision.tsv'
-    }
-    DATASET_MD5 = {'WildVision': 'b38f80156d49411c594772866b0d0b52'}
-
-    score_map = {
-        'A>>B': -2,
-        'A>B': -1,
-        'A=B': 0,
-        'B>A': 1,
-        'B>>A': 2
+        "WildVision": "https://opencompass.openxlab.space/utils/VLMEval/WildVision.tsv"
     }
+    DATASET_MD5 = {"WildVision": "b38f80156d49411c594772866b0d0b52"}
+
+    score_map = {"A>>B": -2, "A>B": -1, "A=B": 0, "B>A": 1, "B>>A": 2}
 
     # Given one data record, return the built prompt (a multi-modal message), can override
     def build_prompt(self, line):
@@ -109,27 +98,27 @@ class WildVision(ImageBaseDataset):
             line = self.data.iloc[line]
 
         if self.meta_only:
-            tgt_path = toliststr(line['image_path'])
+            tgt_path = toliststr(line["image_path"])
         else:
             tgt_path = self.dump_image(line)
 
-        question = line['question']
+        question = line["question"]
 
         msgs = []
         if isinstance(tgt_path, list):
-            msgs.extend([dict(type='image', value=p) for p in tgt_path])
+            msgs.extend([dict(type="image", value=p) for p in tgt_path])
         else:
-            msgs = [dict(type='image', value=tgt_path)]
+            msgs = [dict(type="image", value=tgt_path)]
         # WildVision adopts text first
-        msgs = [dict(type='text', value=question)] + msgs
+        msgs = [dict(type="text", value=question)] + msgs
         return msgs
 
     @classmethod
     def gen_eval_base(self, eval_file, b64_map):
         data = load(eval_file)
-        data['B'] = data.pop('prediction')
-        data['A'] = data.pop('claude3_sonnet')
-        data['image'] = [b64_map[x] for x in data['index']]
+        data["B"] = data.pop("prediction")
+        data["A"] = data.pop("claude3_sonnet")
+        data["image"] = [b64_map[x] for x in data["index"]]
         return data
         # rev = cp.deepcopy(data)
         # rev['A'] = data['B']
@@ -141,30 +130,32 @@ class WildVision(ImageBaseDataset):
     @classmethod
     def evaluate(self, eval_file, **judge_kwargs):
         # We adopt pairwise evaluation (twice for a pair) for this dataset
-        suffix = eval_file.split('.')[-1]
-        model = judge_kwargs['model']
-        storage = eval_file.replace(f'.{suffix}', f'_{model}.xlsx')
-        score_file = eval_file.replace(f'.{suffix}', f'_{model}_score.csv')
-        tmp_file = eval_file.replace(f'.{suffix}', f'_{model}.pkl')
-        nproc = judge_kwargs.pop('nproc', 4)
+        suffix = eval_file.split(".")[-1]
+        model = judge_kwargs["model"]
+        storage = eval_file.replace(f".{suffix}", f"_{model}.xlsx")
+        score_file = eval_file.replace(f".{suffix}", f"_{model}_score.csv")
+        tmp_file = eval_file.replace(f".{suffix}", f"_{model}.pkl")
+        nproc = judge_kwargs.pop("nproc", 4)
 
         if not osp.exists(storage):
-            raw_data = WildVision('WildVision').data
-            b64_map = {x: y for x, y in zip(raw_data['index'], raw_data['image'])}
+            raw_data = WildVision("WildVision").data
+            b64_map = {x: y for x, y in zip(raw_data["index"], raw_data["image"])}
             data = self.gen_eval_base(eval_file, b64_map)
 
-            judge_kwargs['system_prompt'] = SYSTEM_PROMPT
-            judge_kwargs['temperature'] = 0
-            judge_kwargs['img_detail'] = 'high'
-            judge_kwargs['timeout'] = 300
+            judge_kwargs["system_prompt"] = SYSTEM_PROMPT
+            judge_kwargs["temperature"] = 0
+            judge_kwargs["img_detail"] = "high"
+            judge_kwargs["timeout"] = 300
             model = build_judge(max_tokens=4096, **judge_kwargs)
 
-            assert model.working(), ('WildVision evaluation requires a working OPENAI API\n' + DEBUG_MESSAGE)
+            assert model.working(), (
+                "WildVision evaluation requires a working OPENAI API\n" + DEBUG_MESSAGE
+            )
 
             lt = len(data)
             lines = [data.iloc[i] for i in range(lt)]
             tups = [(model, line) for line in lines]
-            indices = [line['index'] for line in lines]
+            indices = [line["index"] for line in lines]
 
             ans = load(tmp_file) if osp.exists(tmp_file) else {}
             tups = [x for x, i in zip(tups, indices) if i not in ans]
@@ -183,8 +174,8 @@ class WildVision(ImageBaseDataset):
                 for k, v in zip(indices, new_results):
                     ans[k] = v
 
-            data['score'] = [ans[idx] for idx in data['index']]
-            data.pop('image')
+            data["score"] = [ans[idx] for idx in data["index"]]
+            data.pop("image")
             dump(data, storage)
 
         data = load(storage)
@@ -193,29 +184,29 @@ class WildVision(ImageBaseDataset):
         scores = defaultdict(lambda: 0)
         for i in range(lt):
             item = data.iloc[i]
-            if item['score'] not in self.score_map:
+            if item["score"] not in self.score_map:
                 score = 0
             else:
-                score = self.score_map[item['score']]
-                if '_rev' in item['index']:
+                score = self.score_map[item["score"]]
+                if "_rev" in item["index"]:
                     score = -score
             scores[score] += 1
         name_map = {
-            2: 'Much Better',
-            1: 'Better',
-            0: 'Tie',
-            -1: 'Worse',
-            -2: 'Much Worse'
+            2: "Much Better",
+            1: "Better",
+            0: "Tie",
+            -1: "Worse",
+            -2: "Much Worse",
         }
         scores = {name_map[k]: v for k, v in scores.items()}
-        much_better = scores.get('Much Better', 0)
-        better = scores.get('Better', 0)
-        worse = scores.get('Worse', 0)
-        much_worse = scores.get('Much Worse', 0)
-        scores['Reward'] = (
+        much_better = scores.get("Much Better", 0)
+        better = scores.get("Better", 0)
+        worse = scores.get("Worse", 0)
+        much_worse = scores.get("Much Worse", 0)
+        scores["Reward"] = (
             100 * much_better + 50 * better - 50 * worse - 100 * much_worse
         ) / lt
-        scores['Win Rate'] = (better + much_better) / lt
+        scores["Win Rate"] = (better + much_better) / lt
         scores = {k: [v] for k, v in scores.items()}
         scores = pd.DataFrame(scores)
         dump(scores, score_file)
-- 
2.42.0

